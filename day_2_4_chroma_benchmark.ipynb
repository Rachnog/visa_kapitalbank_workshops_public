{"cells":[{"cell_type":"code","source":["! pip install langsmith openai langfuse\n","! pip install -qU requests bs4 lxml chromadb langchain langchain-text-splitters langchain-openai\n","! pip install -qU duckduckgo-search langchain-community ddgs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lpnPURKeKJ6U","executionInfo":{"status":"ok","timestamp":1762030409735,"user_tz":-60,"elapsed":42805,"user":{"displayName":"Alex Honchar","userId":"16229384722428101359"}},"outputId":"3bcba23c-bc4b-4406-df9a-0810036cbdd3"},"id":"lpnPURKeKJ6U","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langsmith in /usr/local/lib/python3.12/dist-packages (0.4.38)\n","Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n","Collecting langfuse\n","  Downloading langfuse-3.8.1-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.28.1)\n","Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith) (3.11.4)\n","Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langsmith) (25.0)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.11.10)\n","Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (1.0.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (2.32.5)\n","Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith) (0.25.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n","Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langfuse) (2.2.1)\n","Requirement already satisfied: opentelemetry-api<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.38.0)\n","Requirement already satisfied: opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.37.0)\n","Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.33.1 in /usr/local/lib/python3.12/dist-packages (from langfuse) (1.38.0)\n","Collecting wrapt<2.0,>=1.14 (from langfuse)\n","  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.10.5)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n","Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api<2.0.0,>=1.33.1->langfuse) (8.7.0)\n","Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (1.71.0)\n","Collecting opentelemetry-exporter-otlp-proto-common==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n","  Downloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl.metadata (1.8 kB)\n","Collecting opentelemetry-proto==1.37.0 (from opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse)\n","  Downloading opentelemetry_proto-1.37.0-py3-none-any.whl.metadata (2.3 kB)\n","Collecting opentelemetry-sdk<2.0.0,>=1.33.1 (from langfuse)\n","  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: protobuf<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-proto==1.37.0->opentelemetry-exporter-otlp-proto-http<2.0.0,>=1.33.1->langfuse) (5.29.5)\n","Collecting opentelemetry-api<2.0.0,>=1.33.1 (from langfuse)\n","  Downloading opentelemetry_api-1.37.0-py3-none-any.whl.metadata (1.5 kB)\n","Collecting opentelemetry-semantic-conventions==0.58b0 (from opentelemetry-sdk<2.0.0,>=1.33.1->langfuse)\n","  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl.metadata (2.4 kB)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1->langsmith) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith) (2.3.0)\n","Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api<2.0.0,>=1.33.1->langfuse) (3.23.0)\n","Downloading langfuse-3.8.1-py3-none-any.whl (364 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.6/364.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_common-1.37.0-py3-none-any.whl (18 kB)\n","Downloading opentelemetry_proto-1.37.0-py3-none-any.whl (72 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, opentelemetry-proto, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, opentelemetry-semantic-conventions, opentelemetry-sdk, langfuse\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 2.0.0\n","    Uninstalling wrapt-2.0.0:\n","      Successfully uninstalled wrapt-2.0.0\n","  Attempting uninstall: opentelemetry-proto\n","    Found existing installation: opentelemetry-proto 1.38.0\n","    Uninstalling opentelemetry-proto-1.38.0:\n","      Successfully uninstalled opentelemetry-proto-1.38.0\n","  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n","    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.38.0\n","    Uninstalling opentelemetry-exporter-otlp-proto-common-1.38.0:\n","      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.38.0\n","  Attempting uninstall: opentelemetry-api\n","    Found existing installation: opentelemetry-api 1.38.0\n","    Uninstalling opentelemetry-api-1.38.0:\n","      Successfully uninstalled opentelemetry-api-1.38.0\n","  Attempting uninstall: opentelemetry-semantic-conventions\n","    Found existing installation: opentelemetry-semantic-conventions 0.59b0\n","    Uninstalling opentelemetry-semantic-conventions-0.59b0:\n","      Successfully uninstalled opentelemetry-semantic-conventions-0.59b0\n","  Attempting uninstall: opentelemetry-sdk\n","    Found existing installation: opentelemetry-sdk 1.38.0\n","    Uninstalling opentelemetry-sdk-1.38.0:\n","      Successfully uninstalled opentelemetry-sdk-1.38.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-exporter-otlp-proto-common==1.38.0, but you have opentelemetry-exporter-otlp-proto-common 1.37.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-proto==1.38.0, but you have opentelemetry-proto 1.37.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-grpc 1.38.0 requires opentelemetry-sdk~=1.38.0, but you have opentelemetry-sdk 1.37.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed langfuse-3.8.1 opentelemetry-api-1.37.0 opentelemetry-exporter-otlp-proto-common-1.37.0 opentelemetry-proto-1.37.0 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 wrapt-1.17.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n","google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n","opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import os\n","\n","os.environ[\"OPENAI_API_KEY\"] = \"\""],"metadata":{"id":"e80vt7wEKVv7","executionInfo":{"status":"ok","timestamp":1762030409806,"user_tz":-60,"elapsed":45,"user":{"displayName":"Alex Honchar","userId":"16229384722428101359"}}},"id":"e80vt7wEKVv7","execution_count":2,"outputs":[]},{"cell_type":"code","source":["# kb_en_to_chroma.py  — minimal & direct\n","import os, re, time, requests\n","from urllib.parse import urljoin, urldefrag\n","from bs4 import BeautifulSoup\n","\n","BASE = \"https://www.kapitalbank.az\"\n","START = f\"{BASE}/en\"\n","UA = {\"User-Agent\": \"kb-minicrawl/0.2\"}\n","TIMEOUT = 15\n","MAX_PAGES = 50\n","\n","def clean_url(u):\n","    u = urldefrag(u)[0]\n","    if not u: return None\n","    if not u.startswith(\"http\"): u = urljoin(BASE, u)\n","    if not u.startswith(START): return None\n","    if re.search(r\"\\.(pdf|jpe?g|png|gif|svg|mp4|zip|docx?|xlsx?)$\", u, re.I): return None\n","    return u\n","\n","def extract_text(html):\n","    s = BeautifulSoup(html, \"lxml\")\n","    for t in s([\"script\",\"style\",\"noscript\",\"svg\",\"footer\",\"nav\",\"header\"]): t.decompose()\n","    n = s.select_one(\"main\") or s.select_one(\"article\") or s.body or s\n","    return \" \".join((n.get_text(\" \", strip=True) if n else s.get_text(\" \", strip=True)).split())\n","\n","visited, queue, pages = set(), [START], []\n","while queue and len(visited) < MAX_PAGES:\n","    url = queue.pop(0)\n","    if url in visited: continue\n","    try:\n","        r = requests.get(url, headers=UA, timeout=TIMEOUT)\n","        if r.ok and \"text/html\" in r.headers.get(\"Content-Type\",\"\"):\n","            txt = extract_text(r.text)\n","            if len(txt) > 200:\n","                pages.append({\"url\": url, \"text\": txt})\n","            s = BeautifulSoup(r.text, \"lxml\")\n","            for a in s.find_all(\"a\", href=True):\n","                u = clean_url(a[\"href\"])\n","                if u and u not in visited:\n","                    queue.append(u)\n","        visited.add(url); time.sleep(0.15)\n","    except requests.RequestException:\n","        visited.add(url)\n","\n","import json\n","\n","# Save the crawled pages data to a file for later use\n","pages_outfile = \"kapitalbank_pages.json\"\n","with open(pages_outfile, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(pages, f, indent=2, ensure_ascii=False)\n","print(f\"Saved {len(pages)} pages to {pages_outfile}\")\n","\n","# Load crawled pages from JSON file to make them available for Chroma processing\n","with open(pages_outfile, \"r\", encoding=\"utf-8\") as f:\n","    pages = json.load(f)\n","print(f\"Loaded {len(pages)} pages from {pages_outfile}\")\n","\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","\n","# ---- LangChain chunking ----\n","splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)\n","docs, metas = [], []\n","for p in pages:\n","    for chunk in splitter.split_text(p[\"text\"]):\n","        docs.append(chunk)\n","        metas.append({\"url\": p[\"url\"]})\n","\n","# ---- OpenAI embeddings -> Chroma ----\n","persist_dir = \"chroma_kapitalbank\"\n","emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # cheap & solid\n","vs = Chroma.from_texts(\n","    texts=docs,\n","    embedding=emb,\n","    persist_directory=persist_dir,\n","    collection_name=\"kapitalbank_en\",\n","    metadatas=metas,\n",")\n","vs.persist()\n","print(f\"Indexed pages={len(pages)} chunks={len(docs)} into {persist_dir}/ (collection 'kapitalbank_en')\")\n","\n","from langchain_community.vectorstores import Chroma\n","from langchain_openai import OpenAIEmbeddings\n","\n","persist_dir = \"chroma_kapitalbank\"\n","collection_name = \"kapitalbank_en\"\n","emb = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n","\n","# Load the existing/persisted Chroma vector store\n","vs = Chroma(\n","    persist_directory=persist_dir,\n","    embedding_function=emb,\n","    collection_name=collection_name\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkhh_WwUKWuR","executionInfo":{"status":"ok","timestamp":1762030507877,"user_tz":-60,"elapsed":98044,"user":{"displayName":"Alex Honchar","userId":"16229384722428101359"}},"outputId":"63860c38-789d-43b2-caa4-61a3403f0b9a"},"id":"vkhh_WwUKWuR","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved 39 pages to kapitalbank_pages.json\n","Loaded 39 pages from kapitalbank_pages.json\n","Indexed pages=39 chunks=161 into chroma_kapitalbank/ (collection 'kapitalbank_en')\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2284835735.py:80: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n","  vs.persist()\n","/tmp/ipython-input-2284835735.py:91: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-chroma package and should be used instead. To use it run `pip install -U `langchain-chroma` and import as `from `langchain_chroma import Chroma``.\n","  vs = Chroma(\n"]}]},{"cell_type":"code","source":["# ========= PARAMS (from your command) =========\n","MODE = \"pages\"  # also supports \"newsgroups\" baseline if you toggle it\n","PAGES_FILE = pages_outfile\n","WEBSITE = BASE\n","PERSIST_DIR_STR = persist_dir\n","COLLECTION = collection_name\n","EMBEDDING_MODEL = \"text-embedding-3-small\"\n","CHUNK_SIZE = 800\n","CHUNK_OVERLAP = 120\n","K_LIST = [1, 3, 5, 10]\n","CONCURRENCY = [1, 5, 10]\n","SAMPLE_QUERIES = 300\n","ACL_ALLOW_PREFIXES = [BASE]\n","\n","# Security toggles\n","ENABLE_PII_MASKING = False              # set True to anonymize before ingestion\n","STORE_RAW_HASH = True                   # store hash of raw chunk in metadata (not the raw text)\n","DELETE_PREFIX = None                    # e.g. \"https://www.hsbc.com/media/\" if you want to demo deletion\n","\n","# Outputs\n","MAKE_PLOTS = True\n","MAKE_REPORT = True\n","LLM_MODEL = \"gpt-4o-mini\"               # requires OPENAI_API_KEY in env\n","SHOW_INLINE = False                     # True to display images/HTML inline in notebook\n","\n","# ========= Imports =========\n","import json, math, os, random, re, time, hashlib, statistics, base64\n","from pathlib import Path\n","from dataclasses import dataclass\n","from typing import List, Dict, Any, Tuple, Optional\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","\n","import pandas as pd\n","\n","# plotting (headless save-to-file; will display inline if SHOW_INLINE=True)\n","import matplotlib\n","matplotlib.use(\"Agg\")\n","import matplotlib.pyplot as plt\n","\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_openai import OpenAIEmbeddings\n","from langchain_community.vectorstores import Chroma\n","\n","# (Optional) labeled baseline support\n","try:\n","    from sklearn.datasets import fetch_20newsgroups\n","    from sklearn.utils import Bunch\n","except Exception:\n","    fetch_20newsgroups = None\n","    Bunch = Any  # type: ignore\n","\n","import os\n","from dotenv import load_dotenv\n","\n","load_dotenv(override=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IVHFVN0qJdsc","executionInfo":{"status":"ok","timestamp":1762030507930,"user_tz":-60,"elapsed":34,"user":{"displayName":"Alex Honchar","userId":"16229384722428101359"}},"outputId":"4383ce20-f4e9-4f94-ccdd-07db157bef5c"},"id":"IVHFVN0qJdsc","execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":5,"id":"d04d89c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d04d89c1","executionInfo":{"status":"ok","timestamp":1762030579619,"user_tz":-60,"elapsed":71663,"user":{"displayName":"Alex Honchar","userId":"16229384722428101359"}},"outputId":"0ab03205-4634-41a4-d1f8-23c9fe9025d0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Indexed 161 docs in 2.21s (72.8 docs/s)\n","Disk size: 5.5 MB\n","{\n","  \"concurrency\": 1,\n","  \"q_count\": 161,\n","  \"total_time_s\": 43.62599518100001,\n","  \"throughput_qps\": 3.690460225194329,\n","  \"latency_ms_p50\": 203.354830999956,\n","  \"latency_ms_p95\": 512.1905859999742\n","}\n","{\n","  \"concurrency\": 5,\n","  \"q_count\": 161,\n","  \"total_time_s\": 7.746535279999989,\n","  \"throughput_qps\": 20.783485026611825,\n","  \"latency_ms_p50\": 201.48165899991,\n","  \"latency_ms_p95\": 422.68770099985886\n","}\n","{\n","  \"concurrency\": 10,\n","  \"q_count\": 161,\n","  \"total_time_s\": 3.699653967000131,\n","  \"throughput_qps\": 43.5175833837636,\n","  \"latency_ms_p50\": 196.9967090001319,\n","  \"latency_ms_p95\": 333.61633399999846\n","}\n","Quality (label-free): {\n","  \"SelfRecall@1\": 0.2236024844720497,\n","  \"URLHit@1\": 0.84472049689441,\n","  \"SelfRecall@3\": 0.8695652173913043,\n","  \"URLHit@3\": 0.9627329192546584,\n","  \"SelfRecall@5\": 0.9254658385093167,\n","  \"URLHit@5\": 0.9875776397515528,\n","  \"SelfRecall@10\": 0.9751552795031055,\n","  \"URLHit@10\": 1.0\n","}\n","ACL leakage: {'K': 5, 'checked': 805, 'leakage_rate': 0.0}\n","ACL latency demo: {\n","  \"unfiltered_ms\": 132.85287100006826,\n","  \"filtered_ms\": 132.8787340000872,\n","  \"kept\": 5\n","}\n","\n","Saved summary → chroma_kapitalbank/benchmark_summary.json\n","Saved plots → ['perf_throughput.png', 'perf_latency.png', 'quality.png', 'security_pii.png', 'security_acl.png']\n","Saved LLM report → chroma_kapitalbank/benchmark_explanation.md\n","Saved HTML report → chroma_kapitalbank/benchmark_report.html\n"]}],"source":["# ========= Utilities =========\n","class Timer:\n","    def __enter__(self): self.t0 = time.perf_counter(); return self\n","    def __exit__(self, *exc): self.elapsed = time.perf_counter() - self.t0\n","\n","def percentile(xs: List[float], p: float) -> float:\n","    if not xs: return float(\"nan\")\n","    xs = sorted(xs)\n","    k = (len(xs) - 1) * (p / 100.0)\n","    f, c = math.floor(k), math.ceil(k)\n","    if f == c: return xs[int(k)]\n","    return xs[f] * (c - k) + xs[c] * (k - f)\n","\n","def dir_size_bytes(p: Path) -> int:\n","    total = 0\n","    for root, _, files in os.walk(p):\n","        for f in files:\n","            total += os.path.getsize(os.path.join(root, f))\n","    return total\n","\n","# ========= Security (PII) =========\n","PII_PATTERNS = {\n","    \"EMAIL\": re.compile(r\"\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b\"),\n","    \"PHONE\": re.compile(r\"\\b(?:\\+?\\d{1,3}[\\s.-]?)?(?:\\(?\\d{2,4}\\)?[\\s.-]?){2,4}\\d{2,4}\\b\"),\n","    \"IBAN\": re.compile(r\"\\b[A-Z]{2}\\d{2}[A-Z0-9]{10,30}\\b\"),\n","    \"CREDIT_CARD\": re.compile(r\"\\b(?:\\d[ -]*?){13,19}\\b\"),\n","    # conservative generics; tune per locale\n","    \"GEN_ID\": re.compile(r\"\\b[A-Z]{2}\\d{6}[A-Z]{2}\\b\"),\n","    \"PASSPORT\": re.compile(r\"\\b[A-PR-WY][1-9]\\d{6}\\b\"),\n","}\n","def _hash16(s: str) -> str: return hashlib.sha256((\"salt:\"+s).encode()).hexdigest()[:16]\n","\n","def scan_pii(text: str) -> Dict[str, int]:\n","    out = {k: 0 for k in PII_PATTERNS}\n","    for name, rx in PII_PATTERNS.items():\n","        out[name] = len(list(rx.finditer(text)))\n","    out[\"TOTAL\"] = sum(out.values())\n","    return out\n","\n","def anonymize_text(text: str) -> Tuple[str, Dict[str, int]]:\n","    counts = {k: 0 for k in PII_PATTERNS}\n","    out = text\n","    for name, rx in PII_PATTERNS.items():\n","        def repl(m):\n","            counts[name] += 1\n","            return f\"<{name}_{_hash16(m.group(0))}>\"\n","        out = rx.sub(repl, out)\n","    counts[\"TOTAL\"] = sum(counts.values())\n","    return out, counts\n","\n","# ========= Data models =========\n","@dataclass\n","class QueryResultPages:\n","    lat_ms: float\n","    q_url: str\n","    q_chunk: int\n","    hits: List[Dict[str, Any]]\n","\n","@dataclass\n","class LabeledCorpus:\n","    texts: List[str]\n","    labels: List[int]\n","    label_names: List[str]\n","\n","@dataclass\n","class QueryResultLabeled:\n","    lat_ms: float\n","    ranks: List[int]\n","\n","# ========= Pages (your flow) pipeline =========\n","def load_pages_json(path: Path) -> List[Dict[str, str]]:\n","    with path.open(\"r\") as f:\n","        return json.load(f)\n","\n","def chunk_pages(\n","    pages: List[Dict[str, str]],\n","    chunk_size: int,\n","    chunk_overlap: int,\n","    enable_pii_masking: bool,\n","    store_raw_hash: bool,\n",") -> Tuple[List[str], List[Dict[str, Any]], pd.DataFrame]:\n","    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n","    chunks: List[str] = []; metas: List[Dict[str, Any]] = []; pii_rows: List[Dict[str, Any]] = []\n","    for p in pages:\n","        url, text = p.get(\"url\"), p.get(\"text\", \"\") or \"\"\n","        for i, ch in enumerate(splitter.split_text(text)):\n","            raw = ch\n","            if enable_pii_masking: ch, pii = anonymize_text(raw)\n","            else:                  pii = scan_pii(raw)\n","            meta = {\"url\": url, \"chunk_idx\": i}\n","            if store_raw_hash: meta[\"raw_hash16\"] = _hash16(raw)\n","            chunks.append(ch); metas.append(meta); pii_rows.append({\"url\": url, \"chunk_idx\": i, **pii})\n","    return chunks, metas, pd.DataFrame(pii_rows)\n","\n","def build_index_from_texts(\n","    texts: List[str],\n","    metadatas: List[Dict[str, Any]],\n","    persist_dir: Path,\n","    collection_name: str,\n","    embedding_model: str,\n",") -> Chroma:\n","    emb = OpenAIEmbeddings(model=embedding_model)\n","    with Timer() as t:\n","        vs = Chroma.from_texts(\n","            texts=texts,\n","            embedding=emb,\n","            persist_directory=str(persist_dir),\n","            collection_name=collection_name,\n","            metadatas=metadatas,\n","        )\n","        # Chroma >=0.4 auto-persists; explicit persist() not required\n","    print(f\"Indexed {len(texts)} docs in {t.elapsed:.2f}s ({len(texts)/max(t.elapsed,1e-6):.1f} docs/s)\")\n","    print(f\"Disk size: {dir_size_bytes(persist_dir)/(1024*1024):.1f} MB\")\n","    return vs\n","\n","def single_query_pages(vs: Chroma, query_chunk: str, q_meta: Dict[str, Any], k: int) -> QueryResultPages:\n","    t0 = time.perf_counter()\n","    docs = vs.similarity_search_with_score(query_chunk, k=k)\n","    lat_ms = (time.perf_counter() - t0) * 1000.0\n","    hits = [{\"url\": d.metadata.get(\"url\"), \"chunk_idx\": d.metadata.get(\"chunk_idx\"), \"score\": s} for d, s in docs]\n","    return QueryResultPages(lat_ms=lat_ms, q_url=q_meta[\"url\"], q_chunk=int(q_meta[\"chunk_idx\"]), hits=hits)\n","\n","def compute_pages_quality(results: List[QueryResultPages], ks: List[int]) -> Dict[str, Any]:\n","    out: Dict[str, Any] = {}\n","    for k in ks:\n","        self_hits = 0; url_hits = 0\n","        for r in results:\n","            topk = r.hits[:k]\n","            if any(h[\"url\"] == r.q_url and h[\"chunk_idx\"] == r.q_chunk for h in topk): self_hits += 1\n","            if any(h[\"url\"] == r.q_url for h in topk): url_hits += 1\n","        n = len(results) or 1\n","        out[f\"SelfRecall@{k}\"] = self_hits / n\n","        out[f\"URLHit@{k}\"] = url_hits / n\n","    return out\n","\n","def is_allowed_url(url: str, allow_prefixes: List[str]) -> bool:\n","    return any(url and url.startswith(pref) for pref in allow_prefixes)\n","\n","def acl_leakage_and_latency(vs: Chroma, sample_query_text: str, allow_prefixes: List[str], k_acl: int = 5) -> Dict[str, Any]:\n","    with Timer() as t1:\n","        unfiltered = vs.similarity_search_with_score(sample_query_text, k=k_acl)\n","    with Timer() as t2:  # app-layer post-filter\n","        filtered = [pair for pair in unfiltered if is_allowed_url(pair[0].metadata.get(\"url\", \"\"), allow_prefixes)]\n","    return {\"unfiltered_ms\": t1.elapsed * 1000.0, \"filtered_ms\": (t1.elapsed + t2.elapsed) * 1000.0, \"kept\": len(filtered)}\n","\n","# ========= Labeled (optional baseline) =========\n","def load_newsgroups(train_size: int, test_size: int, seed: int = 42) -> Tuple[LabeledCorpus, LabeledCorpus]:\n","    if fetch_20newsgroups is None:\n","        raise RuntimeError(\"scikit-learn not available; install it or set MODE='pages'\")\n","    random.seed(seed)\n","    train: Bunch = fetch_20newsgroups(subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"))\n","    test:  Bunch = fetch_20newsgroups(subset=\"test\",  remove=(\"headers\", \"footers\", \"quotes\"))\n","    ti = list(range(len(train.data))); xi = list(range(len(test.data)))\n","    random.shuffle(ti); random.shuffle(xi)\n","    ti, xi = ti[:5000], xi[:300]\n","    return (\n","        LabeledCorpus([train.data[i] for i in ti], [int(train.target[i]) for i in ti], list(train.target_names)),\n","        LabeledCorpus([test.data[i]  for i in xi], [int(test.target[i])  for i in xi],  list(test.target_names)),\n","    )\n","\n","# ========= Plotting (fixed) =========\n","def _ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True)\n","\n","def plot_perf(runs: List[Dict[str, Any]], outdir: Path) -> List[Path]:\n","    _ensure_dir(outdir)\n","    conc = [r[\"concurrency\"] for r in runs]\n","    qps  = [r.get(\"throughput_qps\") for r in runs]\n","    p50  = [r.get(\"latency_ms_p50\") for r in runs]\n","    p95  = [r.get(\"latency_ms_p95\") for r in runs]\n","\n","    # QPS\n","    plt.figure()\n","    plt.plot(conc, qps, marker=\"o\")\n","    plt.xlabel(\"Concurrency\"); plt.ylabel(\"Throughput (QPS)\"); plt.title(\"Throughput vs Concurrency\")\n","    p1 = outdir / \"perf_throughput.png\"; plt.savefig(p1, bbox_inches=\"tight\"); plt.close()\n","\n","    # Latency\n","    plt.figure()\n","    plt.plot(conc, p50, marker=\"o\", label=\"p50\")\n","    plt.plot(conc, p95, marker=\"o\", label=\"p95\")\n","    plt.xlabel(\"Concurrency\"); plt.ylabel(\"Latency (ms)\"); plt.title(\"Latency vs Concurrency\"); plt.legend()\n","    p2 = outdir / \"perf_latency.png\"; plt.savefig(p2, bbox_inches=\"tight\"); plt.close()\n","    return [p1, p2]\n","\n","def plot_quality(quality: Dict[str, Any], outdir: Path) -> Path:\n","    _ensure_dir(outdir)\n","    keys = list(quality.keys()); vals = [quality[k] for k in keys]\n","    plt.figure(figsize=(max(6, len(keys)*0.8), 4))\n","    plt.bar(range(len(keys)), vals)\n","    plt.xticks(range(len(keys)), keys, rotation=45, ha=\"right\")\n","    plt.ylim(0, 1.05); plt.ylabel(\"Score\"); plt.title(\"Quality Metrics\")\n","    p = outdir / \"quality.png\"; plt.savefig(p, bbox_inches=\"tight\"); plt.close(); return p\n","\n","def plot_security(pii_summary: Dict[str, Any], leakage_rate: Optional[float], outdir: Path) -> List[Path]:\n","    _ensure_dir(outdir)\n","    # Keep only numeric PII fields; drop aggregate/non-PII keys\n","    clean = {name: val for name, val in pii_summary.items()\n","             if isinstance(val, (int, float)) and name not in (\"TOTAL\", \"chunk_idx\")}\n","    names = list(clean.keys())\n","    counts = [clean[name] for name in names]\n","\n","    # PII bar chart\n","    plt.figure(figsize=(max(6, len(names)*0.8), 4))\n","    plt.bar(range(len(names)), counts)\n","    plt.xticks(range(len(names)), names, rotation=45, ha=\"right\")\n","    plt.ylabel(\"Count\"); plt.title(\"PII occurrences by type\")\n","    p1 = outdir / \"security_pii.png\"; plt.savefig(p1, bbox_inches=\"tight\"); plt.close()\n","\n","    # ACL leakage “card”\n","    plt.figure(figsize=(4, 1.5))\n","    if isinstance(leakage_rate, (int, float)):\n","        txt = f\"ACL leakage rate (top-k): {leakage_rate:.4f}\"\n","    else:\n","        txt = \"ACL leakage rate: n/a\"\n","    plt.text(0.01, 0.5, txt, va=\"center\"); plt.axis(\"off\")\n","    p2 = outdir / \"security_acl.png\"; plt.savefig(p2, bbox_inches=\"tight\"); plt.close()\n","    return [p1, p2]\n","\n","# ========= LLM explanation/report =========\n","def _call_llm_explain(summary: Dict[str, Any], model: str) -> str:\n","    prompt = (\n","        \"You are a critical, no-fluff performance engineer. \"\n","        \"Explain the following Chroma retrieval benchmark summary for a technical audience. \"\n","        \"Be concise, prioritize strategic insights, call out bottlenecks, and give 3–5 concrete next steps. \"\n","        \"Return Markdown. Here is the JSON summary:\\n\\n\"\n","        f\"{json.dumps(summary, indent=2)}\"\n","    )\n","    try:\n","        from openai import OpenAI  # new SDK\n","        client = OpenAI()\n","        resp = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"system\", \"content\": \"You are a terse, incisive benchmarking analyst.\"},\n","                      {\"role\": \"user\", \"content\": prompt}],\n","            temperature=0.2,\n","        )\n","        return resp.choices[0].message.content.strip()\n","    except Exception:\n","        try:\n","            from langfuse.openai import openai  # shim, if you use it\n","            resp = openai.chat.completions.create(\n","                model=model,\n","                messages=[{\"role\": \"system\", \"content\": \"You are a terse, incisive benchmarking analyst.\"},\n","                          {\"role\": \"user\", \"content\": prompt}],\n","                temperature=0.2,\n","            )\n","            return resp.choices[0].message.content.strip()\n","        except Exception as e:\n","            return f\"LLM explanation unavailable. Reason: {e}\"\n","\n","def _img_to_base64(path: Path) -> str:\n","    with path.open(\"rb\") as f:\n","        return base64.b64encode(f.read()).decode(\"utf-8\")\n","\n","def write_reports(persist_dir: Path, summary: Dict[str, Any], llm_md: str, images: List[Path]) -> tuple[Path, Path]:\n","    md_path = persist_dir / \"benchmark_explanation.md\"\n","    html_path = persist_dir / \"benchmark_report.html\"\n","    # Markdown\n","    md_parts = [\n","        \"# Chroma Benchmark Report\", \"\",\n","        \"## Summary JSON\", \"```json\", json.dumps(summary, indent=2), \"```\", \"\",\n","        \"## LLM Explanation\", llm_md or \"*No explanation available.*\", \"\",\n","        \"## Plots\",\n","    ]\n","    for img in images:\n","        md_parts += [f\"### {img.name}\", f\"![{img.name}]({img.name})\", \"\"]\n","    md_path.write_text(\"\\n\".join(md_parts), encoding=\"utf-8\")\n","    # HTML (inline images)\n","    img_blocks = []\n","    for img in images:\n","        try:\n","            b64 = _img_to_base64(img)\n","            img_blocks.append(f'<h3>{img.name}</h3><img alt=\"{img.name}\" src=\"data:image/png;base64,{b64}\" style=\"max-width: 900px;\" />')\n","        except Exception:\n","            img_blocks.append(f'<h3>{img.name}</h3><p>(Could not inline; see file on disk)</p>')\n","    html = f\"\"\"<!doctype html>\n","<html><head><meta charset=\"utf-8\"><title>Chroma Benchmark Report</title></head>\n","<body>\n","<h1>Chroma Benchmark Report</h1>\n","<h2>Summary JSON</h2>\n","<pre>{json.dumps(summary, indent=2)}</pre>\n","<h2>LLM Explanation</h2>\n","<div>{llm_md if llm_md else \"<em>No explanation available.</em>\"}</div>\n","<h2>Plots</h2>\n","{''.join(img_blocks)}\n","</body></html>\"\"\"\n","    html_path.write_text(html, encoding=\"utf-8\")\n","    return md_path, html_path\n","\n","# ========= RUN (pages mode using your parameters) =========\n","random.seed(42)\n","persist_dir = Path(PERSIST_DIR_STR); persist_dir.mkdir(parents=True, exist_ok=True)\n","\n","if MODE == \"pages\":\n","    # 1) Load & chunk (your flow)\n","    pages = load_pages_json(Path(PAGES_FILE))\n","    chunks, metas, pii_df = chunk_pages(\n","        pages=pages,\n","        chunk_size=CHUNK_SIZE,\n","        chunk_overlap=CHUNK_OVERLAP,\n","        enable_pii_masking=ENABLE_PII_MASKING,\n","        store_raw_hash=STORE_RAW_HASH,\n","    )\n","    # 2) Build index\n","    vs = build_index_from_texts(chunks, metas, persist_dir, COLLECTION, EMBEDDING_MODEL)\n","\n","    # 3) Perf: run at given concurrencies\n","    all_idx = list(range(len(chunks))); random.shuffle(all_idx)\n","    q_idx = all_idx[:min(SAMPLE_QUERIES, len(all_idx))]\n","    K = max(K_LIST)\n","\n","    runs = []\n","    for c in CONCURRENCY:\n","        results = []\n","        t0 = time.perf_counter()\n","        if c <= 1:\n","            for i in q_idx:\n","                results.append(single_query_pages(vs, chunks[i], metas[i], K))\n","        else:\n","            with ThreadPoolExecutor(max_workers=c) as ex:\n","                futs = [ex.submit(single_query_pages, vs, chunks[i], metas[i], K) for i in q_idx]\n","                for fut in as_completed(futs):\n","                    results.append(fut.result())\n","        total_s = time.perf_counter() - t0\n","        lats = [r.lat_ms for r in results]\n","        run_metrics = {\n","            \"concurrency\": c,\n","            \"q_count\": len(results),\n","            \"total_time_s\": total_s,\n","            \"throughput_qps\": len(results)/total_s if total_s else None,\n","            \"latency_ms_p50\": statistics.median(lats) if lats else None,\n","            \"latency_ms_p95\": percentile(lats, 95) if lats else None,\n","        }\n","        print(json.dumps(run_metrics, indent=2))\n","        runs.append(run_metrics)\n","\n","    # 4) Quality (label-free) on worst concurrency\n","    worst_c = CONCURRENCY[-1]\n","    worst_results = []\n","    if worst_c <= 1:\n","        for i in q_idx: worst_results.append(single_query_pages(vs, chunks[i], metas[i], K))\n","    else:\n","        with ThreadPoolExecutor(max_workers=worst_c) as ex:\n","            futs = [ex.submit(single_query_pages, vs, chunks[i], metas[i], K) for i in q_idx]\n","            for fut in as_completed(futs): worst_results.append(fut.result())\n","    quality = compute_pages_quality(worst_results, K_LIST)\n","    print(\"Quality (label-free):\", json.dumps(quality, indent=2))\n","\n","    # 5) Security\n","    pii_summary = pii_df.sum(numeric_only=True).to_dict()\n","    K_ACL = min(5, K)\n","    leaks, total_checked = 0, 0\n","    for r in worst_results[:200]:\n","        topk = r.hits[:K_ACL]\n","        for h in topk:\n","            total_checked += 1\n","            if not is_allowed_url(h[\"url\"] or \"\", ACL_ALLOW_PREFIXES): leaks += 1\n","    leakage_rate = (leaks / total_checked) if total_checked else None\n","    print(\"ACL leakage:\", {\"K\": K_ACL, \"checked\": total_checked, \"leakage_rate\": leakage_rate})\n","\n","    acl_demo = None\n","    if worst_results:\n","        acl_demo = acl_leakage_and_latency(vs, chunks[q_idx[0]], ACL_ALLOW_PREFIXES, k_acl=K_ACL)\n","        print(\"ACL latency demo:\", json.dumps(acl_demo, indent=2))\n","\n","    # 6) Optional delete demo\n","    post_delete_offenders = None\n","    if DELETE_PREFIX:\n","        targets = {m[\"url\"] for m in metas if (m.get(\"url\") or \"\").startswith(DELETE_PREFIX)}\n","        for url in targets:\n","            vs._collection.delete(where={\"url\": url})  # NOTE: private attr; demo-only\n","        offenders = 0\n","        for r in worst_results[:100]:\n","            if any((h[\"url\"] or \"\").startswith(DELETE_PREFIX) for h in r.hits): offenders += 1\n","        post_delete_offenders = offenders\n","\n","    # 7) Summary JSON\n","    summary = {\n","        \"index\": {\n","            \"persist_dir\": str(persist_dir),\n","            \"collection\": COLLECTION,\n","            \"embedding_model\": EMBEDDING_MODEL,\n","            \"disk_mb\": round(dir_size_bytes(persist_dir)/(1024*1024), 1),\n","            \"docs\": len(chunks),\n","        },\n","        \"runs\": runs,\n","        \"quality\": quality,\n","        \"security\": {\n","            \"pii_total\": int(pii_summary.get(\"TOTAL\", 0)) if pii_summary else 0,\n","            \"pii_breakdown\": {k:int(v) for k,v in pii_summary.items() if isinstance(v,(int,float))},\n","            \"acl\": {\"allow_prefixes\": ACL_ALLOW_PREFIXES, \"leakage_rate\": leakage_rate, \"latency_demo\": acl_demo},\n","            \"delete_prefix\": DELETE_PREFIX, \"post_delete_offenders\": post_delete_offenders,\n","        }\n","    }\n","else:\n","    # Optional labeled baseline (not used here)\n","    train, test = load_newsgroups(5000, 300)\n","    metas = [{\"topic\": train.label_names[y]} for y in train.labels]\n","    vs = build_index_from_texts(train.texts, metas, Path(PERSIST_DIR_STR), COLLECTION, EMBEDDING_MODEL)\n","    summary = {\"note\": \"labeled mode not executed in this cell\"}\n","\n","# 8) Save summary\n","out_json = Path(PERSIST_DIR_STR) / \"benchmark_summary.json\"\n","out_json.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n","print(f\"\\nSaved summary → {out_json}\")\n","\n","# 9) Plots & report\n","plot_paths: List[Path] = []\n","if MAKE_PLOTS:\n","    plot_paths += plot_perf(summary[\"runs\"], Path(PERSIST_DIR_STR))\n","    if \"quality\" in summary: plot_paths += [plot_quality(summary[\"quality\"], Path(PERSIST_DIR_STR))]\n","    if \"security\" in summary:\n","        piis = summary[\"security\"].get(\"pii_breakdown\", {})\n","        leak = summary[\"security\"].get(\"acl\", {}).get(\"leakage_rate\", None)\n","        plot_paths += plot_security(piis, leak, Path(PERSIST_DIR_STR))\n","    print(f\"Saved plots → {[p.name for p in plot_paths]}\")\n","\n","llm_md = None\n","md_path = html_path = None\n","if MAKE_REPORT:\n","    llm_md = _call_llm_explain(summary, model=LLM_MODEL)\n","    md_path, html_path = write_reports(Path(PERSIST_DIR_STR), summary, llm_md, plot_paths)\n","    print(f\"Saved LLM report → {md_path}\")\n","    print(f\"Saved HTML report → {html_path}\")\n","\n","# 10) Inline display (optional)\n","if SHOW_INLINE:\n","    from IPython.display import display, Image, HTML, Markdown\n","    if plot_paths:\n","        for p in plot_paths:\n","            display(Image(filename=str(p)))\n","    if html_path and html_path.exists():\n","        display(HTML(html_path.read_text()))\n","    elif llm_md:\n","        display(Markdown(llm_md))"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}